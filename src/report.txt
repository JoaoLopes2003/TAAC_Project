1. Data Preparation 

 

Regarding the data preparation we created 2 distinct datasets. This was a decision we made in order to have 2 datasets with 2 different levels of complexety to test our fine tuned models. This way, we started by removing all
the non necessary columns, being those all columns excluding the title, abstract and categories. After this we followed the following paths:

- For the more complex dataset we converted the categories field into a list and after converted it into a one hot encoded matrix. This way we have a matrix with 1s and 0s, where 1 means that the paper belongs to that category
and 0 means that it doesn't.

- For the simpler dataset we determined what were the N most common categories and we filtered the dataset for X registers of each of those N categories. After this we followed the same steps as the previous dataset. This was done to
have a dataset with less categories, to test if the model would perform better with less categories (also facilitating the training process).

After this, we stored the one hot encodings back into a list of binary values, in a column named "labels", in order to facilitate the training process. Addicionally, having in consideration that the model is supposed to predict the
categories based on the abstracts and/or titles, we also substituted the "title" and "abstract" columns by a single column, labeled as "input_text", that folds each row into 3 rows with the same value in the "labels" column, but with
different values in the "input_text" column (one for the title, one for the abstract and one for the concatenation of both). For this we took advantage of markers, with the objective of still identifying what text corresponds to the
title and what text corresponds to the abstract.

Finally, we removed the "title" and "abstract" columns and splited the datasets into train, validation and test datasets, with 80% of the data being used for training and 15% for validation and 5% for testing.

2. Pre-Trained Model Fine Tuning 

Regarding the pre trained models we used, we selected two versions of the BERT pre trained model, one with 262 millions of parameters and another with 60 millions of parameters. We selected these models because they are encoder
only models, which aligns well with our objective of classifying papers. Also we choose two models with different sizes to test if the model with more parameters would perform better than the model with less parameters.

For each model fine tuning we played with parameters such as the learning rate, the batch size, the number of epochs, warmup ratios, weight decay and the usage of the "cosine" for the learning rate scheduler.

2.1. Model 1 - BERT Base Uncased

For the first model we started by selecting only 10 registers and trying to fine tune the model as a base line, training for 3 epochs and following a learning rate of 1e-5 and a batch size of 8. This way, for a test dataset of 300
rows (100 duplicated in 3 rows each), the model was able to correctly predict approximately 160 categories correctly and 15000 wrongly (important to mention that it might have predicted right and wrong categories for the same row).

After this, we fine tuned the model with 200 registers and increased de number of epochs to 5 and the learning rate to 1e-4. This way the model was able to correctly predict approximately 90 categories correctly and 4000 wrongly.

We observed that despite the model guessing wrongly less categories, it was also guessing less categories correctly. This made us think that the model was converging to predict empty category vectors for every input recieved. In order to verify this,
we expanded the dataset to 500 registers and increased the number of epochs to 6, leaving the training runing for 2 hours on Google Colab. As we expected, the model was predicting empty category vectors for every input recieved.

This way we decided to change the model to the BERT Large Uncased model, with 60 millions of parameters, to make it computationaly easier to test different parameters and determine why the model was predicting empty category vectors.

2.2. Model 2 - BERT Large Uncased