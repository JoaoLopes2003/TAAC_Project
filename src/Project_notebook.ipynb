{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Analyzing and Generating Scientific Abstracts**\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "This project explores how Large Language Models (LLMs) can be applied to the analysis and generation of scientific abstracts, using data from the arXiv repository. With the growing volume of scientific publications, LLMs offer the potential to automate abstract and title generation, saving time for researchers. Key tasks include generating titles from abstracts, generating abstracts from titles, and predicting paper categories. The dataset, available on HuggingFace, allows us to test the model's performance and refine it using cross-validation and optimization techniques.\n",
    "\n",
    "## **Use Case**\n",
    "\n",
    "The use case for this project is to help researchers and academics in the scientific community to generate abstracts and titles for their papers. This can be useful for researchers who are looking to quickly generate abstracts for their papers or for students who are looking to learn how to write abstracts. Additionally, the model can be used to predict paper categories, which can help researchers to quickly identify papers that are relevant to their research.\n",
    "\n",
    "## **Scope**\n",
    "\n",
    "This project aims to explore some of the capabilities of NLP models in generating scientific abstracts and titles, taking advantage of the arXiv dataset in order to do that. This way, we aim to provide a tool capable of:\n",
    "\n",
    "- Generating abstracts from titles;\n",
    "- Generating titles from abstracts;\n",
    "- Classifying papers into categories based on their abstracts and/or titles;\n",
    "- Otimizing the model's performance improving the generated titles, abstracts and categories;\n",
    "\n",
    "## **Objectives**\n",
    "\n",
    "The main objectives of this project are:\n",
    "\n",
    "- Develop a tool capable of generating abstracts and titles using LLMs;\n",
    "- Develop a model capable of predicting paper categories based on their abstracts and/or titles;\n",
    "- Apply cross-validation to evaluate the model's accuracy, recall and F1-score;\n",
    "- Improve the model's performance using, for example, alredy existing pre trained models (e.g. BERT, GPT, etc) and fine-tuning them with the arXiv dataset.\n",
    "- EExperiment with summarization models available on HuggingFace and compare their performance.\n",
    "- Analyze the arXiv category structure, propose a hierarchical taxonomy, and implement classification models to assess performance across different levels of this taxonomy.\n",
    "\n",
    "## **Tasks**\n",
    "\n",
    "The tasks involved in this project include:\n",
    "\n",
    "1. **Dataset Analysis**: Analyze the arXiv dataset to understand its structure, metadata, and the distribution of categories. Identify the most relevant features for title generation, abstract generation, and category prediction.\n",
    "\n",
    "2. **Data Preparation**: Preprocess the data for model training and evaluation (e.g., handling missing data, tokenization, train-test split). Create proper datasets for each task (title generation, abstract generation, and classification).\n",
    "\n",
    "3. **Baseline Model Development**: Develop baseline models for generating abstracts and titles and for predicting paper categories using basic architectures like Seq2Seq or LSTM. Evaluate the modelsâ€™ performance using standard metrics (e.g., BLEU, ROUGE, F1).\n",
    "\n",
    "4. **Fine-tuning Pre-trained Models**: Use pre-trained models (e.g., BERT, GPT, T5) and fine-tune them on the arXiv dataset for the tasks of abstract generation, title generation, and category prediction. Evaluate improvements over baseline models.\n",
    "\n",
    "5. **Model Optimization**: Optimize the models through hyperparameter tuning (e.g., learning rate, optimizer choice, number of layers) and by using techniques like early stopping or data augmentation. Consider experimenting with a wider subset of the arXiv dataset.\n",
    "\n",
    "6. **Parameter Tuning for Generation Tasks**: Play with different parameter configurations (e.g., temperature, top-k, top-p) in the title and abstract generation tasks. Compare results to determine optimal settings for quality output.\n",
    "\n",
    "7. **Experiment with Summarization Models**: Test and compare various summarization models from the HuggingFace library (e.g., BART, T5) for abstract generation. Evaluate their performance against metrics like ROUGE and BLEU.\n",
    "\n",
    "8. **Category Prediction and Taxonomy Analysis**: Analyze existing arXiv categories and develop a hierarchical taxonomy. Train models for both flat classification and hierarchical classification. Evaluate performance using metrics like F1-score at different levels of the taxonomy.\n",
    "\n",
    "9. **Cross-Validation and Model Evaluation**: Apply cross-validation to evaluate model robustness. Focus on metrics like accuracy, precision, recall, and F1-score to assess classification models, and BLEU/ROUGE for generation tasks.\n",
    "\n",
    "## **Data Analysis**\n",
    "\n",
    "The dataset contains 1999486 rows and 10 columns. The columns are as follows:\n",
    "\n",
    "- id: Unique identifier of the ArXiv paper;\n",
    "- submitter: Name of the user who submitted the paper;\n",
    "- authors: Authors of the paper;\n",
    "- title: Title of the paper;\n",
    "- comments: Additional comments added to the paper;\n",
    "- journal-ref: Identifier of the journal where the paper was submitted;\n",
    "- doi: Persistent address for the paper;\n",
    "- abstract: Abstract of the paper;\n",
    "- report-no: Unique identifier of the paper within the organization;\n",
    "- categories: Categories associated with the paper.\n",
    "- versions: Version of the paper.\n",
    "\n",
    "In order to analyse the most relevant attributes and how they interact with each other, extracting relevant information from the dataset, we will take advantage of the libraries **pandas** and **ydata_profiling**. These libraries provide a simple way to manipulate and profile datasets and extract relevant information from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "db = pd.read_csv(\"abstracts_trimmed.csv\")\n",
    "\n",
    "# Remove all columns except the abstract, title and categories\n",
    "db = db[[\"abstract\", \"title\", \"categories\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "# Generate the Report\n",
    "profile = ProfileReport(db,title=\"Adult Census Profile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Predicting Paper Categories**\n",
    "\n",
    "First we prepare the data for the classification task, converting the categories column into multiple binary columns, one for each category, and format the data.\n",
    "\n",
    "Here we developed 2 different data preparation methods:\n",
    "\n",
    "- One with all categories;\n",
    "- One with the top N categories in order to reduce the number of classes and check if the model's performance improves.\n",
    "\n",
    "The first one is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "db = pd.read_csv(\"abstracts_trimmed.csv\")\n",
    "\n",
    "# Remove all columns except the abstract, title and categories\n",
    "db = db[[\"abstract\", \"title\", \"categories\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up characters like \"[\", \"]\", \"'\" from the \"categories\" column\n",
    "db[\"categories\"] = db[\"categories\"].str.replace(r\"[\\[\\]']\", \"\", regex=True)\n",
    "\n",
    "# Separate the \"categories\" column into binary columns\n",
    "db = db.join(db[\"categories\"].str.get_dummies(\" \"))\n",
    "\n",
    "# Remove the \"categories\" column\n",
    "db = db.drop(columns=[\"categories\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a small sample of the data , otherwise the training process will be too slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first X rows of the dataset to speed up the process\n",
    "X = 100\n",
    "db2 = db.tail(X)\n",
    "\n",
    "# Save the cleaned dataset\n",
    "db.to_csv(\"predict_dataset.csv\", index=False)\n",
    "\n",
    "# Select the first X rows of the dataset to speed up the process\n",
    "X = 100\n",
    "db = db.head(X)\n",
    "\n",
    "# Save the cleaned dataset\n",
    "db.to_csv(\"abstracts_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second one is the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "db = pd.read_csv(\"/content/sample_data/abstracts_trimmed.csv\")\n",
    "\n",
    "# Remove all columns except the abstract, title and categories\n",
    "db = db[[\"abstract\", \"title\", \"categories\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Assuming db is your original DataFrame containing the categories\n",
    "\n",
    "# Clean up characters like \"[\", \"]\", \"'\" from the \"categories\" column\n",
    "db[\"categories\"] = db[\"categories\"].str.replace(r\"[\\[\\]']\", \"\", regex=True)\n",
    "\n",
    "# Determine the top 20 categories in the dataset\n",
    "all_categories = db[\"categories\"].str.cat(sep=' ').split()\n",
    "category_counts = Counter(all_categories)\n",
    "top_20_categories = [category for category, _ in category_counts.most_common(20)]\n",
    "\n",
    "# Create an empty DataFrame to store the sampled rows\n",
    "normalized_dataset = pd.DataFrame()\n",
    "\n",
    "# Loop through each top category and sample 30 rows\n",
    "for category in top_20_categories:\n",
    "    # Filter rows that contain the current category\n",
    "    filtered_rows = db[db[\"categories\"].str.contains(category)]\n",
    "    \n",
    "    # Ensure the sampled rows contain only top 20 categories\n",
    "    filtered_rows = filtered_rows[filtered_rows[\"categories\"].str.split().apply(lambda x: all(cat in top_20_categories for cat in x))]\n",
    "\n",
    "    # Sample 30 rows if available\n",
    "    if len(filtered_rows) > 30:\n",
    "        sampled_rows = filtered_rows.sample(n=30, random_state=42)\n",
    "    else:\n",
    "        # If less than 30 rows are available, take all of them\n",
    "        sampled_rows = filtered_rows\n",
    "\n",
    "    # Append the sampled rows to the normalized dataset\n",
    "    normalized_dataset = pd.concat([normalized_dataset, sampled_rows], ignore_index=True)\n",
    "\n",
    "# Reset the index of the normalized dataset\n",
    "normalized_dataset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Separate the \"categories\" column into binary columns after normalizing\n",
    "normalized_dataset = normalized_dataset.join(normalized_dataset[\"categories\"].str.get_dummies(\" \"))\n",
    "\n",
    "# Remove the \"categories\" column\n",
    "normalized_dataset = normalized_dataset.drop(columns=[\"categories\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the unique categories in the normalized dataset based on the binary columns\n",
    "unique_categories_normalized = normalized_dataset.columns.tolist()\n",
    "\n",
    "# Count occurrences of each unique category in the normalized dataset\n",
    "normalized_category_counts = normalized_dataset[unique_categories_normalized].sum()\n",
    "\n",
    "# Print the shape of the normalized dataset\n",
    "print(\"Normalized Dataset Shape:\", normalized_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly shuffle the dataset before splitting\n",
    "normalized_dataset = normalized_dataset.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Split the dataset into 90% and 10%\n",
    "train_size = int(0.9 * len(normalized_dataset))\n",
    "train_dataset = normalized_dataset.iloc[:train_size]\n",
    "predict_dataset = normalized_dataset.iloc[train_size:]\n",
    "\n",
    "# Save both datasets to CSV files\n",
    "train_dataset.to_csv('abstracts_cleaned.csv', index=False)\n",
    "predict_dataset.to_csv('predict_dataset.csv', index=False)\n",
    "\n",
    "# Print the shape of the datasets\n",
    "print(\"Training Dataset Shape:\", train_dataset.shape)\n",
    "print(\"Prediction Dataset Shape:\", predict_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning a pre-trained model (BERT) to predict paper categories based on their abstracts and/or titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries to install\n",
    "!pip install datasets\n",
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the dataset to fine-tune the model\n",
    "\n",
    "In order to facilitate the fine-tuning process, we decided to compact all the target columns (cattegories), which corresponded to binary values, into a single column. This column is labeled as \"labels\" and contains a list of binary values, each one corresponding to a category (being 1 if the paper belongs to that category and 0 otherwise).\n",
    "\n",
    "Addicionally, having in consideration that the model is supposed to predict the categories based on the abstracts and/or titles, we also substituted the \"title\" and \"abstract\" columns by a single column, labeled as \"input_text\", that folds each row into 3 rows with the same value in the \"labels\" column, but with different values in the \"input_text\" column (one for the title, one for the abstract and one for the concatenation of both). For this we took advantage of markers, with the objective of still identifying what text corresponds to the title and what text corresponds to the abstract. This way, one row folds in something like the following:\n",
    "\n",
    "| labels | title | abstract |\n",
    "| --- | --- | --- |\n",
    "| [0, 0, ..., 1, 0] | Title of the paper | Abstract of the paper |\n",
    "| ... | ... | ... |\n",
    "\n",
    "Into something like:\n",
    "\n",
    "| labels | input_text |\n",
    "| --- | --- |\n",
    "| [0, 0, ..., 1, 0] | [Title] Title of the paper [Abstract] Abstract of the paper |\n",
    "| [0, 0, ..., 1, 0] | [Title] Title of the paper |\n",
    "| [0, 0, ..., 1, 0] | [Abstract] Abstract of the paper |\n",
    "| ... | ... |\n",
    "\n",
    "This way, we can use the \"input_text\" column as the input for the model and the \"labels\" column as the target, facilitating the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# First load the entire dataset\n",
    "dataset = load_dataset(\"csv\", data_files=\"abstracts_cleaned.csv\")\n",
    "\n",
    "# Define the category columns based on the columns in your dataset\n",
    "category_columns = [col for col in dataset['train'].column_names if col not in [\"title\", \"abstract\"]]\n",
    "\n",
    "# Create the labels column by mapping each category column into a list of binary labels\n",
    "def create_labels(row):\n",
    "    return [float(row[col]) for col in category_columns]\n",
    "\n",
    "# Function to create the 3 required input text variations\n",
    "def expand_rows(batch):\n",
    "    titles = batch[\"title\"]\n",
    "    abstracts = batch[\"abstract\"]\n",
    "    labels_list = batch[\"labels\"]\n",
    "\n",
    "    input_texts = []\n",
    "    labels = []\n",
    "\n",
    "    for title, abstract, label in zip(titles, abstracts, labels_list):\n",
    "        input_texts.extend([\n",
    "            f\"[TITLE] {title} [ABSTRACT] {abstract}\",\n",
    "            f\"[TITLE] {title}\",\n",
    "            f\"[ABSTRACT] {abstract}\"\n",
    "        ])\n",
    "        # Duplicate the label for each variation\n",
    "        labels.extend([label] * 3)\n",
    "\n",
    "    return {\"input_text\": input_texts, \"labels\": labels}\n",
    "\n",
    "# Apply the label creation and remove original category columns\n",
    "dataset = dataset.map(lambda row: {'labels': create_labels(row)})\n",
    "dataset = dataset.remove_columns(category_columns)\n",
    "\n",
    "# Create the train-validation split from the original dataset\n",
    "splits = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Expand rows to have 3 rows for each original row for the training set\n",
    "expanded_train_dataset = splits['train'].map(expand_rows, batched=True, remove_columns=[\"title\", \"abstract\"])\n",
    "\n",
    "# Expand rows to have 3 rows for each original row for the validation set\n",
    "expanded_val_dataset = splits['test'].map(expand_rows, batched=True, remove_columns=[\"title\", \"abstract\"])\n",
    "\n",
    "# Create a new DatasetDict with the desired split names\n",
    "dataset = DatasetDict({\n",
    "    'train': expanded_train_dataset,\n",
    "    'validation': expanded_val_dataset\n",
    "})\n",
    "\n",
    "# Access the train and validation sets\n",
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['validation']\n",
    "\n",
    "# Check the dataset structure\n",
    "print(dataset)\n",
    "\n",
    "# Print some basic information about the splits\n",
    "print(\"\\nTrain dataset size:\", len(train_dataset))\n",
    "print(\"Validation dataset size:\", len(val_dataset))\n",
    "\n",
    "# Print the first example from each split to verify the data\n",
    "print(\"\\nFirst example from train split:\")\n",
    "print(train_dataset[0])\n",
    "print(train_dataset[1])\n",
    "print(train_dataset[2])\n",
    "\n",
    "print(\"\\nFirst example from validation split:\")\n",
    "print(val_dataset[0])\n",
    "print(val_dataset[1])\n",
    "print(val_dataset[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Bert tokenizer to tokenize the input text and prepare the dataset for fine-tuning the model.\n",
    "\n",
    "It's important to mention that we use fine tune tow different BERT models, with different sizes, so it's important to select the right tokenizer for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use the distilbert-base-uncased pre trained model after\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "\n",
    "    return tokenizer(examples[\"input_text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Check the dataset structure\n",
    "print(tokenized_datasets)\n",
    "\n",
    "# Print the first example from each tokenized split to verify the data\n",
    "print(\"\\nFirst example from train split:\")\n",
    "print(tokenized_datasets['train'][0])\n",
    "\n",
    "print(\"\\nFirst example from validation split:\")\n",
    "print(tokenized_datasets['validation'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use the TinyBERT pre trained model after\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "\n",
    "    return tokenizer(examples[\"input_text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Check the dataset structure\n",
    "print(tokenized_datasets)\n",
    "\n",
    "# Print the first example from each tokenized split to verify the data\n",
    "print(\"\\nFirst example from train split:\")\n",
    "print(tokenized_datasets['train'][0])\n",
    "\n",
    "print(\"\\nFirst example from validation split:\")\n",
    "print(tokenized_datasets['validation'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the tokenized datasets by removing the original input_text column\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(\"input_text\")\n",
    "tokenized_datasets = tokenized_datasets.with_format(\"torch\")\n",
    "\n",
    "# Access the train and validation sets\n",
    "print(tokenized_datasets[\"train\"])\n",
    "print(tokenized_datasets[\"validation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "To train the model we used 2 different strategies:\n",
    "\n",
    "- Train the model using cross entropy as loss function;\n",
    "- Train the model using weighted Binary Cross Entropy (BCE) as loss function;\n",
    "\n",
    "Also we used both distilbert-base-uncased and huawei-noah/TinyBERT_General_4L_312D pre trained models.\n",
    "\n",
    "Next we have both training blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using distilbert-base-uncased pre trained model\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "num_labels = len(category_columns)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=num_labels,\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using TinyBERT pre trained model\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "num_labels = len(category_columns)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"huawei-noah/TinyBERT_General_4L_312D\",\n",
    "    num_labels=num_labels,\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using cross entropy as loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=1e-3,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    sigmoid_logits = torch.sigmoid(torch.tensor(logits))\n",
    "    threshold = 0.3\n",
    "    predictions = (sigmoid_logits > threshold).numpy().astype(np.int32)\n",
    "    labels = labels.astype(np.int32)\n",
    "    \n",
    "    accuracies = [accuracy_score(label, pred) for pred, label in zip(predictions, labels)]\n",
    "    return {\"accuracy\": np.mean(accuracies)}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using weighted Binary Cross Entropy (BCE) as loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Calculate appropriate steps based on dataset size\n",
    "total_samples = len(train_dataset)\n",
    "batch_size = 8\n",
    "steps_per_epoch = total_samples // batch_size\n",
    "warmup_steps = steps_per_epoch  # One epoch of warmup\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-4,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    warmup_steps=warmup_steps,\n",
    "    gradient_accumulation_steps=4,\n",
    "    fp16=False,\n",
    "    gradient_checkpointing=False,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    \n",
    "    # Apply sigmoid to get probabilities\n",
    "    probs = 1 / (1 + np.exp(-logits))\n",
    "    \n",
    "    # Lower thresholds for small dataset\n",
    "    thresholds = [0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "    best_f1 = 0\n",
    "    best_threshold = 0.5\n",
    "    best_predictions = None\n",
    "    \n",
    "    # Find the best threshold\n",
    "    for threshold in thresholds:\n",
    "        predictions = (probs > threshold).astype(np.int32)\n",
    "        f1 = f1_score(labels, predictions, average='macro', zero_division=1)\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "            best_predictions = predictions\n",
    "    \n",
    "    # Calculate metrics using the best threshold\n",
    "    predictions = best_predictions\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='macro', zero_division=1\n",
    "    )\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    \n",
    "    # Count positive predictions and actual positives\n",
    "    pred_pos = predictions.sum()\n",
    "    actual_pos = labels.sum()\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'best_threshold': best_threshold,\n",
    "        'predicted_positives': int(pred_pos),\n",
    "        'actual_positives': int(actual_pos)\n",
    "    }\n",
    "\n",
    "class MultilabelTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Weighted BCE loss with higher weight for positive examples\n",
    "        pos_weight = torch.ones_like(labels[0]).float() * 3.0\n",
    "        loss_fct = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        loss = loss_fct(logits.view(-1, logits.shape[-1]), \n",
    "                       labels.float().view(-1, labels.shape[-1]))\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "trainer = MultilabelTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(f\"Training with {total_samples} samples\")\n",
    "print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "print(f\"Warmup steps: {warmup_steps}\")\n",
    "print(f\"Total steps: {steps_per_epoch * training_args.num_train_epochs}\")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model to predict the categories of a new papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the prediction dataset from the csv\n",
    "prediction_dataset = load_dataset(\"csv\", data_files=\"predict_dataset.csv\")\n",
    "\n",
    "# Apply the label creation and remove original category columns\n",
    "prediction_dataset = prediction_dataset.map(lambda row: {'labels': create_labels(row)})\n",
    "prediction_dataset = prediction_dataset.remove_columns(category_columns)\n",
    "\n",
    "# Duplicate the dataset for each input text variation\n",
    "prediction_dataset = prediction_dataset.map(expand_rows, batched=True, remove_columns=[\"title\", \"abstract\"])\n",
    "\n",
    "# Tokenize the prediction dataset\n",
    "tokenized_prediction_dataset = prediction_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Clean up the tokenized prediction dataset by removing the original input_text column\n",
    "tokenized_prediction_dataset = tokenized_prediction_dataset.remove_columns(\"input_text\")\n",
    "\n",
    "# Access the prediction set\n",
    "print(\"Prediction dataset:\\n\")\n",
    "print(tokenized_prediction_dataset)\n",
    "\n",
    "# Print the first example from the prediction set to verify the data\n",
    "print(\"\\nFirst example from prediction set:\")\n",
    "print(tokenized_prediction_dataset[\"train\"][0])\n",
    "print(tokenized_prediction_dataset[\"train\"][1])\n",
    "print(tokenized_prediction_dataset[\"train\"][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the prediction set\n",
    "predictions = trainer.predict(tokenized_prediction_dataset[\"train\"])\n",
    "\n",
    "# Process predictions if necessary\n",
    "predicted_labels = predictions.predictions  # Access the predictions output\n",
    "predicted_categories = (torch.sigmoid(torch.tensor(predicted_labels)) > 0.5).int().numpy()\n",
    "\n",
    "# Function to map binary vectors to category names\n",
    "def get_category_names(binary_vector, category_columns):\n",
    "    return [category_columns[i] for i in range(len(binary_vector)) if binary_vector[i] == 1]\n",
    "\n",
    "# Access the actual labels from the tokenized prediction dataset\n",
    "actual_labels = tokenized_prediction_dataset['train']['labels']\n",
    "\n",
    "total_correct_title_only = 0\n",
    "total_correct_abstract_only = 0\n",
    "total_correct_title_abstract = 0\n",
    "\n",
    "total_wrong_title_only = 0\n",
    "total_wrong_abstract_only = 0\n",
    "total_wrong_title_abstract = 0\n",
    "\n",
    "# Display the expected and predicted categories along with the counts\n",
    "for i in range(len(predicted_categories)):\n",
    "    # Get expected categories from actual labels\n",
    "    expected_categories = get_category_names(actual_labels[i], category_columns)\n",
    "    # Get predicted categories from predicted categories\n",
    "    predicted_category_names = get_category_names(predicted_categories[i], category_columns)\n",
    "    \n",
    "    # Calculate correct and wrong predictions\n",
    "    correct_predictions = set(expected_categories) & set(predicted_category_names)\n",
    "    wrong_predictions = set(predicted_category_names) - set(expected_categories)\n",
    "    num_correct = len(correct_predictions)\n",
    "    num_wrong = len(wrong_predictions)\n",
    "\n",
    "    # Check which input text variations were correctly predicted\n",
    "    if i % 3 == 0:\n",
    "        total_correct_title_abstract += num_correct\n",
    "        total_wrong_title_only += num_wrong\n",
    "    elif i % 3 == 1:\n",
    "        total_correct_title_only += num_correct\n",
    "        total_wrong_abstract_only += num_wrong\n",
    "    else:\n",
    "        total_correct_abstract_only += num_correct\n",
    "        total_wrong_title_abstract += num_wrong\n",
    "\n",
    "    print(f\"Example {i}: Expected categories: {expected_categories}, Predicted categories: {predicted_category_names}\")\n",
    "    print(f\"Correctly predicted: {num_correct}, Incorrectly predicted: {num_wrong}\\n\")\n",
    "\n",
    "# Print the total correct and wrong predictions for each input text variation\n",
    "print(f\"Total correct predictions for title only: {total_correct_title_only}; Total wrong predictions: {total_wrong_title_only}\")\n",
    "print(f\"Total correct predictions for abstract only: {total_correct_abstract_only}; Total wrong predictions: {total_wrong_abstract_only}\")\n",
    "print(f\"Total correct predictions for title and abstract: {total_correct_title_abstract}; Total wrong predictions: {total_wrong_title_abstract}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
