{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Analyzing and Generating Scientific Abstracts**\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "This project explores how Large Language Models (LLMs) can be applied to the analysis and generation of scientific abstracts, using data from the arXiv repository. With the growing volume of scientific publications, LLMs offer the potential to automate abstract and title generation, saving time for researchers. Key tasks include generating titles from abstracts, generating abstracts from titles, and predicting paper categories. The dataset, available on HuggingFace, allows us to test the model's performance and refine it using cross-validation and optimization techniques.\n",
    "\n",
    "## **Use Case**\n",
    "\n",
    "The use case for this project is to help researchers and academics in the scientific community to generate abstracts and titles for their papers. This can be useful for researchers who are looking to quickly generate abstracts for their papers or for students who are looking to learn how to write abstracts. Additionally, the model can be used to predict paper categories, which can help researchers to quickly identify papers that are relevant to their research.\n",
    "\n",
    "## **Scope**\n",
    "\n",
    "This project aims to explore some of the capabilities of NLP models in generating scientific abstracts and titles, taking advantage of the arXiv dataset in order to do that. This way, we aim to provide a tool capable of:\n",
    "\n",
    "- Generating abstracts from titles;\n",
    "- Generating titles from abstracts;\n",
    "- Classifying papers into categories based on their abstracts and/or titles;\n",
    "- Otimizing the model's performance improving the generated titles, abstracts and categories;\n",
    "\n",
    "## **Objectives**\n",
    "\n",
    "The main objectives of this project are:\n",
    "\n",
    "- Develop a tool capable of generating abstracts and titles using LLMs;\n",
    "- Develop a model capable of predicting paper categories based on their abstracts and/or titles;\n",
    "- Apply cross-validation to evaluate the model's accuracy, recall and F1-score;\n",
    "- Improve the model's performance using, for example, alredy existing pre trained models (e.g. BERT, GPT, etc) and fine-tuning them with the arXiv dataset.\n",
    "- EExperiment with summarization models available on HuggingFace and compare their performance.\n",
    "- Analyze the arXiv category structure, propose a hierarchical taxonomy, and implement classification models to assess performance across different levels of this taxonomy.\n",
    "\n",
    "## **Tasks**\n",
    "\n",
    "The tasks involved in this project include:\n",
    "\n",
    "1. **Dataset Analysis**: Analyze the arXiv dataset to understand its structure, metadata, and the distribution of categories. Identify the most relevant features for title generation, abstract generation, and category prediction.\n",
    "\n",
    "2. **Data Preparation**: Preprocess the data for model training and evaluation (e.g., handling missing data, tokenization, train-test split). Create proper datasets for each task (title generation, abstract generation, and classification).\n",
    "\n",
    "3. **Baseline Model Development**: Develop baseline models for generating abstracts and titles and for predicting paper categories using basic architectures like Seq2Seq or LSTM. Evaluate the models’ performance using standard metrics (e.g., BLEU, ROUGE, F1).\n",
    "\n",
    "4. **Fine-tuning Pre-trained Models**: Use pre-trained models (e.g., BERT, GPT, T5) and fine-tune them on the arXiv dataset for the tasks of abstract generation, title generation, and category prediction. Evaluate improvements over baseline models.\n",
    "\n",
    "5. **Model Optimization**: Optimize the models through hyperparameter tuning (e.g., learning rate, optimizer choice, number of layers) and by using techniques like early stopping or data augmentation. Consider experimenting with a wider subset of the arXiv dataset.\n",
    "\n",
    "6. **Parameter Tuning for Generation Tasks**: Play with different parameter configurations (e.g., temperature, top-k, top-p) in the title and abstract generation tasks. Compare results to determine optimal settings for quality output.\n",
    "\n",
    "7. **Experiment with Summarization Models**: Test and compare various summarization models from the HuggingFace library (e.g., BART, T5) for abstract generation. Evaluate their performance against metrics like ROUGE and BLEU.\n",
    "\n",
    "8. **Category Prediction and Taxonomy Analysis**: Analyze existing arXiv categories and develop a hierarchical taxonomy. Train models for both flat classification and hierarchical classification. Evaluate performance using metrics like F1-score at different levels of the taxonomy.\n",
    "\n",
    "9. **Cross-Validation and Model Evaluation**: Apply cross-validation to evaluate model robustness. Focus on metrics like accuracy, precision, recall, and F1-score to assess classification models, and BLEU/ROUGE for generation tasks.\n",
    "\n",
    "## **Data Analysis**\n",
    "\n",
    "The dataset contains 1999486 rows and 10 columns. The columns are as follows:\n",
    "\n",
    "- id: Unique identifier of the ArXiv paper;\n",
    "- submitter: Name of the user who submitted the paper;\n",
    "- authors: Authors of the paper;\n",
    "- title: Title of the paper;\n",
    "- comments: Additional comments added to the paper;\n",
    "- journal-ref: Identifier of the journal where the paper was submitted;\n",
    "- doi: Persistent address for the paper;\n",
    "- abstract: Abstract of the paper;\n",
    "- report-no: Unique identifier of the paper within the organization;\n",
    "- categories: Categories associated with the paper.\n",
    "- versions: Version of the paper.\n",
    "\n",
    "In order to analyse the most relevant attributes and how they interact with each other, extracting relevant information from the dataset, we will take advantage of the libraries **pandas** and **ydata_profiling**. These libraries provide a simple way to manipulate and profile datasets and extract relevant information from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "db = pd.read_csv(\"abstracts_trimmed.csv\")\n",
    "\n",
    "# Remove all columns except the abstract, title and categories\n",
    "db = db[[\"abstract\", \"title\", \"categories\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "# Generate the Report\n",
    "profile = ProfileReport(db,title=\"Adult Census Profile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Predicting Paper Titles from Abstracts**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As long as the training is concered, we tried two different approaches. A first one, where we chose to split the data between training and evaluation in a more “fixed” way, randomly sampling 20% for test, and 80% for training, a slightly different approach from the more common 30-70 which proved to be better results than the latter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by defining our device, and using the **datasets** lib to load and sample 1% of the original dataset, in order to speed the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\", pad_token_id=tokenizer.eos_token_id).to(device)\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"gfissore/arxiv-abstracts-2021\", split=\"train[:1%]\")  # Load only 1% of the dataset\n",
    "df = dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must also define a class to be used by the refered lib to help handle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Dataset class\n",
    "class TitleSummarizationDataset(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs['input_ids'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.inputs['input_ids'][idx],\n",
    "            'attention_mask': self.inputs['attention_mask'][idx],\n",
    "            'labels': self.targets['input_ids'][idx]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we devide the dataset into training and testing as described before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue with the rest of your code\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then convert the dataframe into a dictionary of mappings between inputs and targets, and generate the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "def preprocess_data(dataframe):\n",
    "    training_data = []\n",
    "    for _, row in dataframe.iterrows():\n",
    "        training_data.append({\n",
    "            'input': row['abstract'],\n",
    "            'target': row['title']\n",
    "        })\n",
    "    return training_data\n",
    "\n",
    "# Process training and testing data\n",
    "training_data = preprocess_data(train_df)\n",
    "testing_data = preprocess_data(test_df)\n",
    "\n",
    "train_inputs = tokenizer([item['input'] for item in training_data], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "train_targets = tokenizer([item['target'] for item in training_data], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "test_inputs = tokenizer([item['input'] for item in testing_data], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "test_targets = tokenizer([item['target'] for item in testing_data], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = TitleSummarizationDataset(train_inputs, train_targets)\n",
    "test_dataset = TitleSummarizationDataset(test_inputs, test_targets)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can train and save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "# Save model and tokenizer\n",
    "model.save_pretrained(\"fine_tuned_t5\")\n",
    "tokenizer.save_pretrained(\"fine_tuned_t5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this model didn't give us excellent results, we decided to change the way training and testing data is sampled to use k-folds.\n",
    "Also, we decided to try few-shot learning, by giving the model 10 exemples for each fold. We alto tried different epoch numbers and batches combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold cross-validation setup\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# K-Fold cross-validation\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(df)):\n",
    "    print(f\"\\nFold {fold + 1}\")\n",
    "\n",
    "    # Select few samples for few-shot learning (e.g., use only 10 samples for training in this fold)\n",
    "    few_shot_train_idx = train_idx[:10]  # Change the number of samples here if needed\n",
    "    train_df = df.iloc[few_shot_train_idx]\n",
    "    val_df = df.iloc[val_idx]\n",
    "\n",
    "    # Process training and validation data\n",
    "    training_data = preprocess_data(train_df)\n",
    "    validation_data = preprocess_data(val_df)\n",
    "\n",
    "    train_inputs = tokenizer([item['input'] for item in training_data], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    train_targets = tokenizer([item['target'] for item in training_data], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    val_inputs = tokenizer([item['input'] for item in validation_data], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    val_targets = tokenizer([item['target'] for item in validation_data], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = TitleSummarizationDataset(train_inputs, train_targets)\n",
    "    val_dataset = TitleSummarizationDataset(val_inputs, val_targets)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=8)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "   \n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_dataloader, desc=\"Training\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f'Average Training Loss for Fold {fold + 1}: {avg_train_loss:.4f}')\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_dataloader, desc=\"Evaluating\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            total_val_loss += outputs.loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    print(f'Average Validation Loss for Fold {fold + 1}: {avg_val_loss:.4f}')\n",
    "\n",
    "# Save the model and tokenizer after the last fold\n",
    "model.save_pretrained(\"fine_tuned_t5_kfold\")\n",
    "tokenizer.save_pretrained(\"fine_tuned_t5_kfold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After evaluation, we conclued our model performance was quite satisfactory.\n",
    "For example, our model predicted:\n",
    "**Pure spinor formulation of the ten-dimensional superstring leads to manifestly supersymmetric loop amplitudes** for groundtruth **Fermionic superstring loop amplitudes in the pure spinor formalism**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, it performed satisfactory qualitatively-wise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Predicting Paper Abstracts from Titles**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given its impressive abilities to generate text, we decided to try gpt2, despite being “older”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AdamW\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split  # Import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we decided to sample 1000 lines from the original dataset. We must be extra careful not to make padding tokens fall in the evaluation criteria, since there is such a big discrepancy between input and target character sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\"\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Use eos_token as pad_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"abstracts_trimmed.csv\",nrows=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitleAbstractGenerationDataset(Dataset):\n",
    "    def __init__(self, inputs, targets, max_length=512):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs['input_ids'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.inputs['input_ids'][idx]\n",
    "        attention_mask = self.inputs['attention_mask'][idx]\n",
    "        labels = self.targets['input_ids'][idx]\n",
    "        \n",
    "        # Ensure proper padding handling\n",
    "        labels = torch.tensor([\n",
    "            label if mask == 1 else -100\n",
    "            for label, mask in zip(labels, self.targets['attention_mask'][idx])\n",
    "        ])\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "    \n",
    "# Preprocess data\n",
    "def preprocess_data(dataframe):\n",
    "    training_data = []\n",
    "    for _, row in dataframe.iterrows():\n",
    "        formatted_input = f\"Title: {row['title']}\\nGenerate abstract:\"\n",
    "        training_data.append({\n",
    "            'input': formatted_input,\n",
    "            'target': row['abstract']\n",
    "        })\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the nature of this problem, we tried multiple combinations of training hyperparameters, until we achieved this final state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = {\n",
    "    'batch_size': 8,  \n",
    "    'learning_rate': 2e-5,  \n",
    "    'max_length': 512,  \n",
    "    'warmup_steps': 500,\n",
    "    'weight_decay': 0.01,\n",
    "    'num_epochs': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation sets (80-20 split)\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Process training and validation data\n",
    "training_data = preprocess_data(train_df)\n",
    "validation_data = preprocess_data(val_df)\n",
    "\n",
    "# Tokenize inputs and targets\n",
    "train_inputs = tokenizer([item['input'] for item in training_data], return_tensors=\"pt\", padding=\"max_length\", truncation=True).to(device)\n",
    "train_targets = tokenizer([item['target'] for item in training_data], return_tensors=\"pt\", padding=\"max_length\", truncation=True).to(device)\n",
    "val_inputs = tokenizer([item['input'] for item in validation_data], return_tensors=\"pt\", padding=\"max_length\", truncation=True).to(device)\n",
    "val_targets = tokenizer([item['target'] for item in validation_data], return_tensors=\"pt\", padding=\"max_length\", truncation=True).to(device)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = TitleAbstractGenerationDataset(train_inputs, train_targets)\n",
    "val_dataset = TitleAbstractGenerationDataset(val_inputs, val_targets)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=3, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "total_loss = 0\n",
    "for batch in tqdm(train_dataloader, desc=\"Training\"):\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    outputs = model(input_ids=batch['input_ids'], labels=batch['labels'], attention_mask=batch['attention_mask'])\n",
    "    loss = outputs.loss\n",
    "    total_loss += loss.item()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "avg_train_loss = total_loss / len(train_dataloader)\n",
    "print(f'Average Training Loss: {avg_train_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate it, printing the values for average loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation loop\n",
    "model.eval()\n",
    "total_val_loss = 0\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_dataloader, desc=\"Evaluating\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(input_ids=batch['input_ids'], labels=batch['labels'], attention_mask=batch['attention_mask'])\n",
    "        total_val_loss += outputs.loss.item()\n",
    "\n",
    "avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "print(f'Average Validation Loss: {avg_val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, finally, saving the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"fine_tuned_gpt2_train_test_split\")\n",
    "tokenizer.save_pretrained(\"fine_tuned_gpt2_train_test_split\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we quickly realized that \tthe fact that gpt2 is so much bigger in terms of parameters (137M compared to ~60M of t5) could be a disadvantage, since it normally requires a really big number of training data, and therefore, computacional power and time, to train a model with so many parameters. \n",
    "\n",
    "##\n",
    "\n",
    "#### **Gpt2’s output** ####\n",
    "\n",
    "“s. of the of. are. for. in. the. is. to. with. and. at. by. which.in.of.the. that. we.and.with.that. were. also. as. have.to., and, of, in the are, are the and arethe, to, theof,and theand, which are are and the, for, we are to thein, is, as,is, by,theandis.. We are also the in, with, that is the the a. a the , of and in are by the to and that the is and of are that,of the for and by are both.” "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided then to move **back to t5-small**, since not only is it faster to train, but also it is more suitable for our problem, since GPT-2 was trained for autoregressive text generation, not specifically for transforming one text format to another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class TitleAbstractDataset(Dataset):\n",
    "    def __init__(self, titles, abstracts, tokenizer, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.titles = titles\n",
    "        self.abstracts = abstracts\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.titles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Format prompting text\n",
    "        input_text = f\"generate scientific abstract: {self.titles[idx]}\"\n",
    "        target_text = self.abstracts[idx]\n",
    "\n",
    "        # Tokenize inputs\n",
    "        inputs = self.tokenizer(\n",
    "            input_text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Tokenize targets\n",
    "        targets = self.tokenizer(\n",
    "            target_text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs.input_ids.squeeze(),\n",
    "            'attention_mask': inputs.attention_mask.squeeze(),\n",
    "            'labels': targets.input_ids.squeeze(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we really had to test inumerous combinations of training hyperparameters and used the learning rate at our advantage to save the best model developed througout the different epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_t5_model():\n",
    "    # Initialize model and tokenizer\n",
    "    model_name = \"t5-small\"\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "    \n",
    "    # Device setup\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = {\n",
    "        'batch_size': 8,\n",
    "        'learning_rate': 5e-5,  \n",
    "        'max_length': 512,\n",
    "        'num_epochs': 5,\n",
    "        'weight_decay': 0.01,\n",
    "    }\n",
    "    \n",
    "    # Load data\n",
    "    df = pd.read_csv(\"abstracts_trimmed.csv\",nrows=100)\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = TitleAbstractDataset(\n",
    "        train_df['title'].tolist(),\n",
    "        train_df['abstract'].tolist(),\n",
    "        tokenizer\n",
    "    )\n",
    "    \n",
    "    val_dataset = TitleAbstractDataset(\n",
    "        val_df['title'].tolist(),\n",
    "        val_df['abstract'].tolist(),\n",
    "        tokenizer\n",
    "    )\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=training_args['batch_size'],\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=training_args['batch_size']\n",
    "    )\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=training_args['learning_rate'],\n",
    "        weight_decay=training_args['weight_decay']\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(training_args['num_epochs']):\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(train_dataloader, desc=f'Epoch {epoch+1}')\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=batch['input_ids'],\n",
    "                attention_mask=batch['attention_mask'],\n",
    "                labels=batch['labels']\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_dataloader, desc='Validating'):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                outputs = model(\n",
    "                    input_ids=batch['input_ids'],\n",
    "                    attention_mask=batch['attention_mask'],\n",
    "                    labels=batch['labels']\n",
    "                )\n",
    "                total_val_loss += outputs.loss.item()\n",
    "        \n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "        print(f'Epoch {epoch+1}:')\n",
    "        print(f'Average Training Loss: {avg_train_loss:.4f}')\n",
    "        print(f'Average Validation Loss: {avg_val_loss:.4f}')\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            model.save_pretrained(\"fine_tuned_t5_small\")\n",
    "            tokenizer.save_pretrained(\"fine_tuned_t5_small\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task, the prompting template is quite relevant to retrieve the best final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_abstract(title, model, tokenizer, device):\n",
    "    input_text = f\"generate scientific abstract: {title}\"\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512,\n",
    "        truncation=True\n",
    "    ).to(device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=320,\n",
    "        min_length=100,  \n",
    "        num_beams=4,\n",
    "        length_penalty=2.0,\n",
    "        no_repeat_ngram_size=3,\n",
    "        early_stopping=True,\n",
    "        temperature=0.6,\n",
    "        top_k=50,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.2 \n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then craeted a method to train and a basic I/O to test the model with cherry-picked inputs, in order to help visualize its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_abstract(title, model, tokenizer, device):\n",
    "    input_text = f\"generate scientific abstract: {title}\"\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512,\n",
    "        truncation=True\n",
    "    ).to(device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=400,\n",
    "        min_length=130,\n",
    "        num_beams=4,\n",
    "        length_penalty=2.0,\n",
    "        no_repeat_ngram_size=3,\n",
    "        early_stopping=True,\n",
    "        temperature=0.6,\n",
    "        top_k=50,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.7\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_path = \"fine_tuned_t5_small\"\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path).to(device)\n",
    "\n",
    "test_title = \"Calculation of prompt diphoton production cross sections at Tevatron and LHC energies\"\n",
    "generated_abstract = generate_abstract(test_title, model, tokenizer, device)\n",
    "print(\"\\nGenerated Abstract:\")\n",
    "print(generated_abstract)\n",
    "\n",
    "while True:\n",
    "    print(\"\\nEnter a title (or 'quit' to exit):\")\n",
    "    title = input()\n",
    "    if title.lower() == 'quit':\n",
    "        break\n",
    "    \n",
    "    abstract = generate_abstract(title, model, tokenizer, device)\n",
    "    print(\"\\nGenerated Abstract:\")\n",
    "    print(abstract)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this final model is much closer to what we know as Natural Language:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generated Abstract:**\n",
    "In this paper, we present a model of the diphoton production cross sections at Tevatron and LHC energies. It is possible to calculate the number of short-term diphotones in the DHC energy (Tevatron) or LHC energy. The results are presented herein. We provide an example of the effect of a long-term relationship between the two types of diphotonosis. We discuss the effects of the interferometry of the transferometric properties of the recombinantly, we propose a method for determining the distribution of the deposition of the time-to-diphoton output cross sections on the tevatron power using a combination of these two methods. We have shown that it is possible that there is a significant difference in the density of the NHC energy by a fraction of the SHC energy with a large proportion of the subgroups of the E."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Predicting Paper Categories**\n",
    "\n",
    "First we prepare the data for the classification task, converting the categories column into multiple binary columns, one for each category, and format the data.\n",
    "\n",
    "Here we developed 2 different data preparation methods:\n",
    "\n",
    "- One with all categories;\n",
    "- One with the top N categories in order to reduce the number of classes and check if the model's performance improves.\n",
    "\n",
    "The first one is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "db = pd.read_csv(\"abstracts_trimmed.csv\")\n",
    "\n",
    "# Remove all columns except the abstract, title and categories\n",
    "db = db[[\"abstract\", \"title\", \"categories\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up characters like \"[\", \"]\", \"'\" from the \"categories\" column\n",
    "db[\"categories\"] = db[\"categories\"].str.replace(r\"[\\[\\]']\", \"\", regex=True)\n",
    "\n",
    "# Separate the \"categories\" column into binary columns\n",
    "db = db.join(db[\"categories\"].str.get_dummies(\" \"))\n",
    "\n",
    "# Remove the \"categories\" column\n",
    "db = db.drop(columns=[\"categories\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a small sample of the data , otherwise the training process will be too slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first X rows of the dataset to speed up the process\n",
    "X = 100\n",
    "db2 = db.tail(X)\n",
    "\n",
    "# Save the cleaned dataset\n",
    "db.to_csv(\"predict_dataset.csv\", index=False)\n",
    "\n",
    "# Select the first X rows of the dataset to speed up the process\n",
    "X = 100\n",
    "db = db.head(X)\n",
    "\n",
    "# Save the cleaned dataset\n",
    "db.to_csv(\"abstracts_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second one is the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "db = pd.read_csv(\"/content/sample_data/abstracts_trimmed.csv\")\n",
    "\n",
    "# Remove all columns except the abstract, title and categories\n",
    "db = db[[\"abstract\", \"title\", \"categories\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Assuming db is your original DataFrame containing the categories\n",
    "\n",
    "# Clean up characters like \"[\", \"]\", \"'\" from the \"categories\" column\n",
    "db[\"categories\"] = db[\"categories\"].str.replace(r\"[\\[\\]']\", \"\", regex=True)\n",
    "\n",
    "# Determine the top 20 categories in the dataset\n",
    "all_categories = db[\"categories\"].str.cat(sep=' ').split()\n",
    "category_counts = Counter(all_categories)\n",
    "top_20_categories = [category for category, _ in category_counts.most_common(20)]\n",
    "\n",
    "# Create an empty DataFrame to store the sampled rows\n",
    "normalized_dataset = pd.DataFrame()\n",
    "\n",
    "# Loop through each top category and sample 30 rows\n",
    "for category in top_20_categories:\n",
    "    # Filter rows that contain the current category\n",
    "    filtered_rows = db[db[\"categories\"].str.contains(category)]\n",
    "    \n",
    "    # Ensure the sampled rows contain only top 20 categories\n",
    "    filtered_rows = filtered_rows[filtered_rows[\"categories\"].str.split().apply(lambda x: all(cat in top_20_categories for cat in x))]\n",
    "\n",
    "    # Sample 30 rows if available\n",
    "    if len(filtered_rows) > 30:\n",
    "        sampled_rows = filtered_rows.sample(n=30, random_state=42)\n",
    "    else:\n",
    "        # If less than 30 rows are available, take all of them\n",
    "        sampled_rows = filtered_rows\n",
    "\n",
    "    # Append the sampled rows to the normalized dataset\n",
    "    normalized_dataset = pd.concat([normalized_dataset, sampled_rows], ignore_index=True)\n",
    "\n",
    "# Reset the index of the normalized dataset\n",
    "normalized_dataset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Separate the \"categories\" column into binary columns after normalizing\n",
    "normalized_dataset = normalized_dataset.join(normalized_dataset[\"categories\"].str.get_dummies(\" \"))\n",
    "\n",
    "# Remove the \"categories\" column\n",
    "normalized_dataset = normalized_dataset.drop(columns=[\"categories\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the unique categories in the normalized dataset based on the binary columns\n",
    "unique_categories_normalized = normalized_dataset.columns.tolist()\n",
    "\n",
    "# Count occurrences of each unique category in the normalized dataset\n",
    "normalized_category_counts = normalized_dataset[unique_categories_normalized].sum()\n",
    "\n",
    "# Print the shape of the normalized dataset\n",
    "print(\"Normalized Dataset Shape:\", normalized_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly shuffle the dataset before splitting\n",
    "normalized_dataset = normalized_dataset.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Split the dataset into 90% and 10%\n",
    "train_size = int(0.9 * len(normalized_dataset))\n",
    "train_dataset = normalized_dataset.iloc[:train_size]\n",
    "predict_dataset = normalized_dataset.iloc[train_size:]\n",
    "\n",
    "# Save both datasets to CSV files\n",
    "train_dataset.to_csv('abstracts_cleaned.csv', index=False)\n",
    "predict_dataset.to_csv('predict_dataset.csv', index=False)\n",
    "\n",
    "# Print the shape of the datasets\n",
    "print(\"Training Dataset Shape:\", train_dataset.shape)\n",
    "print(\"Prediction Dataset Shape:\", predict_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning a pre-trained model (BERT) to predict paper categories based on their abstracts and/or titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries to install\n",
    "!pip install datasets\n",
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the dataset to fine-tune the model\n",
    "\n",
    "In order to facilitate the fine-tuning process, we decided to compact all the target columns (cattegories), which corresponded to binary values, into a single column. This column is labeled as \"labels\" and contains a list of binary values, each one corresponding to a category (being 1 if the paper belongs to that category and 0 otherwise).\n",
    "\n",
    "Addicionally, having in consideration that the model is supposed to predict the categories based on the abstracts and/or titles, we also substituted the \"title\" and \"abstract\" columns by a single column, labeled as \"input_text\", that folds each row into 3 rows with the same value in the \"labels\" column, but with different values in the \"input_text\" column (one for the title, one for the abstract and one for the concatenation of both). For this we took advantage of markers, with the objective of still identifying what text corresponds to the title and what text corresponds to the abstract. This way, one row folds in something like the following:\n",
    "\n",
    "| labels | title | abstract |\n",
    "| --- | --- | --- |\n",
    "| [0, 0, ..., 1, 0] | Title of the paper | Abstract of the paper |\n",
    "| ... | ... | ... |\n",
    "\n",
    "Into something like:\n",
    "\n",
    "| labels | input_text |\n",
    "| --- | --- |\n",
    "| [0, 0, ..., 1, 0] | [Title] Title of the paper [Abstract] Abstract of the paper |\n",
    "| [0, 0, ..., 1, 0] | [Title] Title of the paper |\n",
    "| [0, 0, ..., 1, 0] | [Abstract] Abstract of the paper |\n",
    "| ... | ... |\n",
    "\n",
    "This way, we can use the \"input_text\" column as the input for the model and the \"labels\" column as the target, facilitating the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# First load the entire dataset\n",
    "dataset = load_dataset(\"csv\", data_files=\"abstracts_cleaned.csv\")\n",
    "\n",
    "# Define the category columns based on the columns in your dataset\n",
    "category_columns = [col for col in dataset['train'].column_names if col not in [\"title\", \"abstract\"]]\n",
    "\n",
    "# Create the labels column by mapping each category column into a list of binary labels\n",
    "def create_labels(row):\n",
    "    return [float(row[col]) for col in category_columns]\n",
    "\n",
    "# Function to create the 3 required input text variations\n",
    "def expand_rows(batch):\n",
    "    titles = batch[\"title\"]\n",
    "    abstracts = batch[\"abstract\"]\n",
    "    labels_list = batch[\"labels\"]\n",
    "\n",
    "    input_texts = []\n",
    "    labels = []\n",
    "\n",
    "    for title, abstract, label in zip(titles, abstracts, labels_list):\n",
    "        input_texts.extend([\n",
    "            f\"[TITLE] {title} [ABSTRACT] {abstract}\",\n",
    "            f\"[TITLE] {title}\",\n",
    "            f\"[ABSTRACT] {abstract}\"\n",
    "        ])\n",
    "        # Duplicate the label for each variation\n",
    "        labels.extend([label] * 3)\n",
    "\n",
    "    return {\"input_text\": input_texts, \"labels\": labels}\n",
    "\n",
    "# Apply the label creation and remove original category columns\n",
    "dataset = dataset.map(lambda row: {'labels': create_labels(row)})\n",
    "dataset = dataset.remove_columns(category_columns)\n",
    "\n",
    "# Create the train-validation split from the original dataset\n",
    "splits = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Expand rows to have 3 rows for each original row for the training set\n",
    "expanded_train_dataset = splits['train'].map(expand_rows, batched=True, remove_columns=[\"title\", \"abstract\"])\n",
    "\n",
    "# Expand rows to have 3 rows for each original row for the validation set\n",
    "expanded_val_dataset = splits['test'].map(expand_rows, batched=True, remove_columns=[\"title\", \"abstract\"])\n",
    "\n",
    "# Create a new DatasetDict with the desired split names\n",
    "dataset = DatasetDict({\n",
    "    'train': expanded_train_dataset,\n",
    "    'validation': expanded_val_dataset\n",
    "})\n",
    "\n",
    "# Access the train and validation sets\n",
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['validation']\n",
    "\n",
    "# Check the dataset structure\n",
    "print(dataset)\n",
    "\n",
    "# Print some basic information about the splits\n",
    "print(\"\\nTrain dataset size:\", len(train_dataset))\n",
    "print(\"Validation dataset size:\", len(val_dataset))\n",
    "\n",
    "# Print the first example from each split to verify the data\n",
    "print(\"\\nFirst example from train split:\")\n",
    "print(train_dataset[0])\n",
    "print(train_dataset[1])\n",
    "print(train_dataset[2])\n",
    "\n",
    "print(\"\\nFirst example from validation split:\")\n",
    "print(val_dataset[0])\n",
    "print(val_dataset[1])\n",
    "print(val_dataset[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Bert tokenizer to tokenize the input text and prepare the dataset for fine-tuning the model.\n",
    "\n",
    "It's important to mention that we use fine tune tow different BERT models, with different sizes, so it's important to select the right tokenizer for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use the distilbert-base-uncased pre trained model after\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "\n",
    "    return tokenizer(examples[\"input_text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Check the dataset structure\n",
    "print(tokenized_datasets)\n",
    "\n",
    "# Print the first example from each tokenized split to verify the data\n",
    "print(\"\\nFirst example from train split:\")\n",
    "print(tokenized_datasets['train'][0])\n",
    "\n",
    "print(\"\\nFirst example from validation split:\")\n",
    "print(tokenized_datasets['validation'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use the TinyBERT pre trained model after\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "\n",
    "    return tokenizer(examples[\"input_text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Check the dataset structure\n",
    "print(tokenized_datasets)\n",
    "\n",
    "# Print the first example from each tokenized split to verify the data\n",
    "print(\"\\nFirst example from train split:\")\n",
    "print(tokenized_datasets['train'][0])\n",
    "\n",
    "print(\"\\nFirst example from validation split:\")\n",
    "print(tokenized_datasets['validation'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the tokenized datasets by removing the original input_text column\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(\"input_text\")\n",
    "tokenized_datasets = tokenized_datasets.with_format(\"torch\")\n",
    "\n",
    "# Access the train and validation sets\n",
    "print(tokenized_datasets[\"train\"])\n",
    "print(tokenized_datasets[\"validation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "To train the model we used 2 different strategies:\n",
    "\n",
    "- Train the model using cross entropy as loss function;\n",
    "- Train the model using weighted Binary Cross Entropy (BCE) as loss function;\n",
    "\n",
    "Also we used both distilbert-base-uncased and huawei-noah/TinyBERT_General_4L_312D pre trained models.\n",
    "\n",
    "Next we have both training blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using distilbert-base-uncased pre trained model\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "num_labels = len(category_columns)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=num_labels,\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using TinyBERT pre trained model\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "num_labels = len(category_columns)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"huawei-noah/TinyBERT_General_4L_312D\",\n",
    "    num_labels=num_labels,\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using cross entropy as loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=1e-3,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    sigmoid_logits = torch.sigmoid(torch.tensor(logits))\n",
    "    threshold = 0.3\n",
    "    predictions = (sigmoid_logits > threshold).numpy().astype(np.int32)\n",
    "    labels = labels.astype(np.int32)\n",
    "    \n",
    "    accuracies = [accuracy_score(label, pred) for pred, label in zip(predictions, labels)]\n",
    "    return {\"accuracy\": np.mean(accuracies)}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using weighted Binary Cross Entropy (BCE) as loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Calculate appropriate steps based on dataset size\n",
    "total_samples = len(train_dataset)\n",
    "batch_size = 8\n",
    "steps_per_epoch = total_samples // batch_size\n",
    "warmup_steps = steps_per_epoch  # One epoch of warmup\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-4,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    warmup_steps=warmup_steps,\n",
    "    gradient_accumulation_steps=4,\n",
    "    fp16=False,\n",
    "    gradient_checkpointing=False,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    \n",
    "    # Apply sigmoid to get probabilities\n",
    "    probs = 1 / (1 + np.exp(-logits))\n",
    "    \n",
    "    # Lower thresholds for small dataset\n",
    "    thresholds = [0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "    best_f1 = 0\n",
    "    best_threshold = 0.5\n",
    "    best_predictions = None\n",
    "    \n",
    "    # Find the best threshold\n",
    "    for threshold in thresholds:\n",
    "        predictions = (probs > threshold).astype(np.int32)\n",
    "        f1 = f1_score(labels, predictions, average='macro', zero_division=1)\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "            best_predictions = predictions\n",
    "    \n",
    "    # Calculate metrics using the best threshold\n",
    "    predictions = best_predictions\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='macro', zero_division=1\n",
    "    )\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    \n",
    "    # Count positive predictions and actual positives\n",
    "    pred_pos = predictions.sum()\n",
    "    actual_pos = labels.sum()\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'best_threshold': best_threshold,\n",
    "        'predicted_positives': int(pred_pos),\n",
    "        'actual_positives': int(actual_pos)\n",
    "    }\n",
    "\n",
    "class MultilabelTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Weighted BCE loss with higher weight for positive examples\n",
    "        pos_weight = torch.ones_like(labels[0]).float() * 3.0\n",
    "        loss_fct = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        loss = loss_fct(logits.view(-1, logits.shape[-1]), \n",
    "                       labels.float().view(-1, labels.shape[-1]))\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "trainer = MultilabelTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(f\"Training with {total_samples} samples\")\n",
    "print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "print(f\"Warmup steps: {warmup_steps}\")\n",
    "print(f\"Total steps: {steps_per_epoch * training_args.num_train_epochs}\")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model to predict the categories of a new papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the prediction dataset from the csv\n",
    "prediction_dataset = load_dataset(\"csv\", data_files=\"predict_dataset.csv\")\n",
    "\n",
    "# Apply the label creation and remove original category columns\n",
    "prediction_dataset = prediction_dataset.map(lambda row: {'labels': create_labels(row)})\n",
    "prediction_dataset = prediction_dataset.remove_columns(category_columns)\n",
    "\n",
    "# Duplicate the dataset for each input text variation\n",
    "prediction_dataset = prediction_dataset.map(expand_rows, batched=True, remove_columns=[\"title\", \"abstract\"])\n",
    "\n",
    "# Tokenize the prediction dataset\n",
    "tokenized_prediction_dataset = prediction_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Clean up the tokenized prediction dataset by removing the original input_text column\n",
    "tokenized_prediction_dataset = tokenized_prediction_dataset.remove_columns(\"input_text\")\n",
    "\n",
    "# Access the prediction set\n",
    "print(\"Prediction dataset:\\n\")\n",
    "print(tokenized_prediction_dataset)\n",
    "\n",
    "# Print the first example from the prediction set to verify the data\n",
    "print(\"\\nFirst example from prediction set:\")\n",
    "print(tokenized_prediction_dataset[\"train\"][0])\n",
    "print(tokenized_prediction_dataset[\"train\"][1])\n",
    "print(tokenized_prediction_dataset[\"train\"][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the prediction set\n",
    "predictions = trainer.predict(tokenized_prediction_dataset[\"train\"])\n",
    "\n",
    "# Process predictions if necessary\n",
    "predicted_labels = predictions.predictions  # Access the predictions output\n",
    "predicted_categories = (torch.sigmoid(torch.tensor(predicted_labels)) > 0.5).int().numpy()\n",
    "\n",
    "# Function to map binary vectors to category names\n",
    "def get_category_names(binary_vector, category_columns):\n",
    "    return [category_columns[i] for i in range(len(binary_vector)) if binary_vector[i] == 1]\n",
    "\n",
    "# Access the actual labels from the tokenized prediction dataset\n",
    "actual_labels = tokenized_prediction_dataset['train']['labels']\n",
    "\n",
    "total_correct_title_only = 0\n",
    "total_correct_abstract_only = 0\n",
    "total_correct_title_abstract = 0\n",
    "\n",
    "total_wrong_title_only = 0\n",
    "total_wrong_abstract_only = 0\n",
    "total_wrong_title_abstract = 0\n",
    "\n",
    "# Display the expected and predicted categories along with the counts\n",
    "for i in range(len(predicted_categories)):\n",
    "    # Get expected categories from actual labels\n",
    "    expected_categories = get_category_names(actual_labels[i], category_columns)\n",
    "    # Get predicted categories from predicted categories\n",
    "    predicted_category_names = get_category_names(predicted_categories[i], category_columns)\n",
    "    \n",
    "    # Calculate correct and wrong predictions\n",
    "    correct_predictions = set(expected_categories) & set(predicted_category_names)\n",
    "    wrong_predictions = set(predicted_category_names) - set(expected_categories)\n",
    "    num_correct = len(correct_predictions)\n",
    "    num_wrong = len(wrong_predictions)\n",
    "\n",
    "    # Check which input text variations were correctly predicted\n",
    "    if i % 3 == 0:\n",
    "        total_correct_title_abstract += num_correct\n",
    "        total_wrong_title_only += num_wrong\n",
    "    elif i % 3 == 1:\n",
    "        total_correct_title_only += num_correct\n",
    "        total_wrong_abstract_only += num_wrong\n",
    "    else:\n",
    "        total_correct_abstract_only += num_correct\n",
    "        total_wrong_title_abstract += num_wrong\n",
    "\n",
    "    print(f\"Example {i}: Expected categories: {expected_categories}, Predicted categories: {predicted_category_names}\")\n",
    "    print(f\"Correctly predicted: {num_correct}, Incorrectly predicted: {num_wrong}\\n\")\n",
    "\n",
    "# Print the total correct and wrong predictions for each input text variation\n",
    "print(f\"Total correct predictions for title only: {total_correct_title_only}; Total wrong predictions: {total_wrong_title_only}\")\n",
    "print(f\"Total correct predictions for abstract only: {total_correct_abstract_only}; Total wrong predictions: {total_wrong_abstract_only}\")\n",
    "print(f\"Total correct predictions for title and abstract: {total_correct_title_abstract}; Total wrong predictions: {total_wrong_title_abstract}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
